{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 6787_project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SG2cB0S-NCGR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2b2b862-601c-4148-b116-e27b6a947416"
      },
      "source": [
        "!pip install tensorflow_federated"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_federated\n",
            "  Downloading tensorflow_federated-0.19.0-py2.py3-none-any.whl (602 kB)\n",
            "\u001b[K     |████████████████████████████████| 602 kB 26.8 MB/s \n",
            "\u001b[?25hCollecting grpcio~=1.34.0\n",
            "  Downloading grpcio-1.34.1-cp37-cp37m-manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 48.6 MB/s \n",
            "\u001b[?25hCollecting cachetools~=3.1.1\n",
            "  Downloading cachetools-3.1.1-py2.py3-none-any.whl (11 kB)\n",
            "Collecting attrs~=19.3.0\n",
            "  Downloading attrs-19.3.0-py2.py3-none-any.whl (39 kB)\n",
            "Collecting tensorflow~=2.5.0\n",
            "  Downloading tensorflow-2.5.2-cp37-cp37m-manylinux2010_x86_64.whl (454.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 454.4 MB 24 kB/s \n",
            "\u001b[?25hRequirement already satisfied: retrying~=1.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow_federated) (1.3.3)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow_federated) (1.19.5)\n",
            "Collecting semantic-version~=2.8.5\n",
            "  Downloading semantic_version-2.8.5-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: portpicker~=1.3.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow_federated) (1.3.9)\n",
            "Requirement already satisfied: jax~=0.2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow_federated) (0.2.25)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow_federated) (0.12.0)\n",
            "Requirement already satisfied: jaxlib~=0.1.55 in /usr/local/lib/python3.7/dist-packages (from tensorflow_federated) (0.1.71+cuda111)\n",
            "Collecting tensorflow-model-optimization~=0.5.0\n",
            "  Downloading tensorflow_model_optimization-0.5.0-py2.py3-none-any.whl (172 kB)\n",
            "\u001b[K     |████████████████████████████████| 172 kB 68.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow_federated) (0.1.6)\n",
            "Collecting tensorflow-privacy~=0.5.0\n",
            "  Downloading tensorflow_privacy-0.5.2-py3-none-any.whl (192 kB)\n",
            "\u001b[K     |████████████████████████████████| 192 kB 74.3 MB/s \n",
            "\u001b[?25hCollecting tqdm~=4.28.1\n",
            "  Downloading tqdm-4.28.1-py2.py3-none-any.whl (45 kB)\n",
            "\u001b[K     |████████████████████████████████| 45 kB 2.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py~=0.10->tensorflow_federated) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jax~=0.2.8->tensorflow_federated) (3.10.0.2)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax~=0.2.8->tensorflow_federated) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from jax~=0.2.8->tensorflow_federated) (1.4.1)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from jaxlib~=0.1.55->tensorflow_federated) (2.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.5.0->tensorflow_federated) (0.37.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.5.0->tensorflow_federated) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.5.0->tensorflow_federated) (3.17.3)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.5.0->tensorflow_federated) (0.4.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.5.0->tensorflow_federated) (1.6.3)\n",
            "Collecting tensorflow-estimator<2.6.0,>=2.5.0\n",
            "  Downloading tensorflow_estimator-2.5.0-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 55.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.5.0->tensorflow_federated) (3.1.0)\n",
            "Collecting wrapt~=1.12.1\n",
            "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.5.0->tensorflow_federated) (1.1.0)\n",
            "Collecting keras-nightly~=2.5.0.dev\n",
            "  Downloading keras_nightly-2.5.0.dev2021032900-py2.py3-none-any.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 67.1 MB/s \n",
            "\u001b[?25hCollecting typing-extensions\n",
            "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.5.0->tensorflow_federated) (2.7.0)\n",
            "Collecting flatbuffers<3.0,>=1.12\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.5.0->tensorflow_federated) (0.2.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow~=2.5.0->tensorflow_federated) (1.5.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated) (3.3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated) (1.8.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated) (1.0.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated) (3.1.1)\n",
            "Requirement already satisfied: mpmath in /usr/local/lib/python3.7/dist-packages (from tensorflow-privacy~=0.5.0->tensorflow_federated) (1.2.1)\n",
            "Building wheels for collected packages: wrapt\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl size=68723 sha256=a1a5a5bf3a69bb3f581377467864f3a0171788db6bfed4dbbca11655e334ebd8\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\n",
            "Successfully built wrapt\n",
            "Installing collected packages: typing-extensions, cachetools, grpcio, wrapt, tensorflow-estimator, keras-nightly, flatbuffers, tqdm, tensorflow-privacy, tensorflow-model-optimization, tensorflow, semantic-version, attrs, tensorflow-federated\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 3.10.0.2\n",
            "    Uninstalling typing-extensions-3.10.0.2:\n",
            "      Successfully uninstalled typing-extensions-3.10.0.2\n",
            "  Attempting uninstall: cachetools\n",
            "    Found existing installation: cachetools 4.2.4\n",
            "    Uninstalling cachetools-4.2.4:\n",
            "      Successfully uninstalled cachetools-4.2.4\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.42.0\n",
            "    Uninstalling grpcio-1.42.0:\n",
            "      Successfully uninstalled grpcio-1.42.0\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.13.3\n",
            "    Uninstalling wrapt-1.13.3:\n",
            "      Successfully uninstalled wrapt-1.13.3\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.7.0\n",
            "    Uninstalling tensorflow-estimator-2.7.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 2.0\n",
            "    Uninstalling flatbuffers-2.0:\n",
            "      Successfully uninstalled flatbuffers-2.0\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.62.3\n",
            "    Uninstalling tqdm-4.62.3:\n",
            "      Successfully uninstalled tqdm-4.62.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.7.0\n",
            "    Uninstalling tensorflow-2.7.0:\n",
            "      Successfully uninstalled tensorflow-2.7.0\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 21.2.0\n",
            "    Uninstalling attrs-21.2.0:\n",
            "      Successfully uninstalled attrs-21.2.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 2.2.4 requires tqdm<5.0.0,>=4.38.0, but you have tqdm 4.28.1 which is incompatible.\n",
            "pymc3 3.11.4 requires cachetools>=4.2.1, but you have cachetools 3.1.1 which is incompatible.\n",
            "panel 0.12.1 requires tqdm>=4.48.0, but you have tqdm 4.28.1 which is incompatible.\n",
            "fbprophet 0.7.1 requires tqdm>=4.36.1, but you have tqdm 4.28.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed attrs-19.3.0 cachetools-3.1.1 flatbuffers-1.12 grpcio-1.34.1 keras-nightly-2.5.0.dev2021032900 semantic-version-2.8.5 tensorflow-2.5.2 tensorflow-estimator-2.5.0 tensorflow-federated-0.19.0 tensorflow-model-optimization-0.5.0 tensorflow-privacy-0.5.2 tqdm-4.28.1 typing-extensions-3.7.4.3 wrapt-1.12.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Uc9OKCENkcT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22152c86-aa56-41fb-9def-4488334a875e"
      },
      "source": [
        "!pip install git+https://github.com/adap/flower.git@release/0.17#egg=flwr[\"simulation\"]  # For a specific branch (release/0.17) w/ extra (\"simulation\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flwr[simulation]\n",
            "  Cloning https://github.com/adap/flower.git (to revision release/0.17) to /tmp/pip-install-_pvr1qxl/flwr_db75f9946e5f41f1a9bd5db8cd0644c1\n",
            "  Running command git clone -q https://github.com/adap/flower.git /tmp/pip-install-_pvr1qxl/flwr_db75f9946e5f41f1a9bd5db8cd0644c1\n",
            "  Running command git checkout -b release/0.17 --track origin/release/0.17\n",
            "  Switched to a new branch 'release/0.17'\n",
            "  Branch 'release/0.17' set up to track remote branch 'release/0.17' from 'origin'.\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: protobuf<4.0.0,>=3.12.1 in /usr/local/lib/python3.7/dist-packages (from flwr[simulation]) (3.17.3)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.19.0 in /usr/local/lib/python3.7/dist-packages (from flwr[simulation]) (1.19.5)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.27.2 in /usr/local/lib/python3.7/dist-packages (from flwr[simulation]) (1.34.1)\n",
            "Requirement already satisfied: google<3.0.0,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from flwr[simulation]) (2.0.3)\n",
            "Collecting ray[default]==1.6.0\n",
            "  Downloading ray-1.6.0-cp37-cp37m-manylinux2014_x86_64.whl (49.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 49.6 MB 38 kB/s \n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray[default]==1.6.0->flwr[simulation]) (7.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray[default]==1.6.0->flwr[simulation]) (3.4.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray[default]==1.6.0->flwr[simulation]) (3.13)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray[default]==1.6.0->flwr[simulation]) (1.0.3)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from ray[default]==1.6.0->flwr[simulation]) (19.3.0)\n",
            "Collecting redis>=3.5.0\n",
            "  Downloading redis-4.0.2-py3-none-any.whl (119 kB)\n",
            "\u001b[K     |████████████████████████████████| 119 kB 57.9 MB/s \n",
            "\u001b[?25hCollecting aiohttp-cors\n",
            "  Downloading aiohttp_cors-0.7.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from ray[default]==1.6.0->flwr[simulation]) (0.12.0)\n",
            "Collecting gpustat\n",
            "  Downloading gpustat-0.6.0.tar.gz (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 7.3 MB/s \n",
            "\u001b[?25hCollecting opencensus\n",
            "  Downloading opencensus-0.8.0-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[K     |████████████████████████████████| 128 kB 58.0 MB/s \n",
            "\u001b[?25hCollecting py-spy>=0.2.0\n",
            "  Downloading py_spy-0.3.11-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 48.8 MB/s \n",
            "\u001b[?25hCollecting colorful\n",
            "  Downloading colorful-0.5.4-py2.py3-none-any.whl (201 kB)\n",
            "\u001b[K     |████████████████████████████████| 201 kB 59.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray[default]==1.6.0->flwr[simulation]) (2.6.0)\n",
            "Collecting aioredis<2\n",
            "  Downloading aioredis-1.3.1-py3-none-any.whl (65 kB)\n",
            "\u001b[K     |████████████████████████████████| 65 kB 3.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ray[default]==1.6.0->flwr[simulation]) (2.23.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 26.2 MB/s \n",
            "\u001b[?25hCollecting hiredis\n",
            "  Downloading hiredis-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl (85 kB)\n",
            "\u001b[K     |████████████████████████████████| 85 kB 3.1 MB/s \n",
            "\u001b[?25hCollecting async-timeout\n",
            "  Downloading async_timeout-4.0.1-py3-none-any.whl (5.7 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from google<3.0.0,>=2.0.3->flwr[simulation]) (4.6.3)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio<2.0.0,>=1.27.2->flwr[simulation]) (1.15.0)\n",
            "Collecting deprecated\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 52.6 MB/s \n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB)\n",
            "\u001b[K     |████████████████████████████████| 192 kB 64.1 MB/s \n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 49.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->ray[default]==1.6.0->flwr[simulation]) (3.7.4.3)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->ray[default]==1.6.0->flwr[simulation]) (2.0.8)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp->ray[default]==1.6.0->flwr[simulation]) (2.10)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated->redis>=3.5.0->ray[default]==1.6.0->flwr[simulation]) (1.12.1)\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.7/dist-packages (from gpustat->ray[default]==1.6.0->flwr[simulation]) (7.352.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from gpustat->ray[default]==1.6.0->flwr[simulation]) (5.4.8)\n",
            "Collecting blessings>=1.6\n",
            "  Downloading blessings-1.7-py3-none-any.whl (18 kB)\n",
            "Collecting opencensus-context==0.1.2\n",
            "  Downloading opencensus_context-0.1.2-py2.py3-none-any.whl (4.4 kB)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from opencensus->ray[default]==1.6.0->flwr[simulation]) (1.26.3)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]==1.6.0->flwr[simulation]) (21.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]==1.6.0->flwr[simulation]) (1.53.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]==1.6.0->flwr[simulation]) (57.4.0)\n",
            "Requirement already satisfied: google-auth<2.0dev,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]==1.6.0->flwr[simulation]) (1.35.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]==1.6.0->flwr[simulation]) (2018.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]==1.6.0->flwr[simulation]) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]==1.6.0->flwr[simulation]) (3.1.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]==1.6.0->flwr[simulation]) (4.8)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]==1.6.0->flwr[simulation]) (3.0.6)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.21.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]==1.6.0->flwr[simulation]) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ray[default]==1.6.0->flwr[simulation]) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ray[default]==1.6.0->flwr[simulation]) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->ray[default]==1.6.0->flwr[simulation]) (3.0.4)\n",
            "Building wheels for collected packages: flwr, gpustat\n",
            "  Building wheel for flwr (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flwr: filename=flwr-0.17.0-py3-none-any.whl size=229390 sha256=e93a268058b12455b43eaab51a03fcd976fcddb21e260b5b1ca670ffaf80536d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-1k215x5_/wheels/f0/40/ba/b2c4b9c5bd10d62b8517b18a4f46f2f60860b861e22997709b\n",
            "  Building wheel for gpustat (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpustat: filename=gpustat-0.6.0-py3-none-any.whl size=12617 sha256=83afcf502b39b7c9507ce7f940c45136ad10c9943cd90dd93814e6ec24f1d663\n",
            "  Stored in directory: /root/.cache/pip/wheels/e6/67/af/f1ad15974b8fd95f59a63dbf854483ebe5c7a46a93930798b8\n",
            "Successfully built flwr gpustat\n",
            "Installing collected packages: multidict, frozenlist, yarl, deprecated, asynctest, async-timeout, aiosignal, redis, opencensus-context, hiredis, blessings, aiohttp, ray, py-spy, opencensus, gpustat, colorful, aioredis, aiohttp-cors, flwr\n",
            "Successfully installed aiohttp-3.8.1 aiohttp-cors-0.7.0 aioredis-1.3.1 aiosignal-1.2.0 async-timeout-4.0.1 asynctest-0.13.0 blessings-1.7 colorful-0.5.4 deprecated-1.2.13 flwr-0.17.0 frozenlist-1.2.0 gpustat-0.6.0 hiredis-2.0.0 multidict-5.2.0 opencensus-0.8.0 opencensus-context-0.1.2 py-spy-0.3.11 ray-1.6.0 redis-4.0.2 yarl-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUMXQK6SRHg4",
        "outputId": "c9f52ea1-5e4d-4fa8-c4f6-39633ef47bc4"
      },
      "source": [
        "!pip install tensorflow_privacy\n",
        "from tensorflow_privacy.privacy.optimizers.dp_optimizer_keras_vectorized import (\n",
        "    VectorizedDPKerasAdamOptimizer, VectorizedDPKerasSGDOptimizer\n",
        ")\n",
        "\n",
        "#RUNTIME ENVIRONMENT MUST BE NONE/CPU\n",
        "\n",
        "#Dummy Training Parameters \n",
        "#Microbatch_Num and Noise are used for Differential Privacy\n",
        "#Make sure MicroBatch_Num evenly divides Batch_Size\n",
        "#Higher values of Noise result in much slower training--needs to be investigated\n",
        "#Haven't experimented with microbatch_num size yet. Below is from TensorFlow description of microbatch_num parameter\n",
        "\n",
        "\"\"\"microbatches (int) - Each batch of data is split in smaller units called microbatches. \n",
        "By default, each microbatch should contain a single training example. \n",
        "This allows us to clip gradients on a per-example basis rather than after they have been averaged across the minibatch. \n",
        "This in turn decreases the (negative) effect of clipping on signal found in the gradient and typically maximizes utility.\n",
        " However, computational overhead can be reduced by increasing the size of microbatches to include more than one training examples. \n",
        "The average gradient across these multiple training examples is then clipped.\n",
        "The total number of examples consumed in a batch, i.e., one step of gradient descent, remains the same. \n",
        "The number of microbatches should evenly divide the batch size.\"\"\"\n",
        "\n",
        "\n",
        "import tensorflow_privacy"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow_privacy in /usr/local/lib/python3.7/dist-packages (0.5.2)\n",
            "Requirement already satisfied: scipy>=0.17 in /usr/local/lib/python3.7/dist-packages (from tensorflow_privacy) (1.4.1)\n",
            "Requirement already satisfied: mpmath in /usr/local/lib/python3.7/dist-packages (from tensorflow_privacy) (1.2.1)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow_privacy) (0.1.6)\n",
            "Requirement already satisfied: tensorflow-estimator>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_privacy) (2.5.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from dm-tree~=0.1.1->tensorflow_privacy) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy>=0.17->tensorflow_privacy) (1.19.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2zL2OWqTmo1",
        "outputId": "5d3fd3ef-5b08-4f55-8093-e6e8a6202445"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6X973gPGgLVa"
      },
      "source": [
        "#imports\n",
        "import os\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "\n",
        "import tensorflow_federated as tff\n",
        "from datetime import datetime\n",
        "import collections\n",
        "import tensorflow as tf\n",
        "import math\n",
        "import numpy as np\n",
        "from typing import Any, Callable, Dict, List, Optional, Tuple\n",
        "import pandas as pd\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
        "import tensorflow_privacy\n",
        "from time import time\n",
        "\n",
        "import flwr as fl"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlWSwf_pTpWM"
      },
      "source": [
        "#data processing\n",
        "@tf.autograph.experimental.do_not_convert\n",
        "def preprocess(dataset, batch_size):\n",
        "\n",
        "  def batch_format_fn(element):\n",
        "    \"\"\"Flatten a batch `pixels` and return the features as an `OrderedDict`.\"\"\"\n",
        "    return collections.OrderedDict(\n",
        "        x=tf.reshape(element['pixels'], [-1, 28,28]),\n",
        "        y=tf.reshape(element['label'], [-1, 1]))\n",
        "\n",
        "  return dataset.shuffle(SHUFFLE_BUFFER, seed=1).batch(\n",
        "      batch_size).map(batch_format_fn).prefetch(PREFETCH_BUFFER)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJd198wCTgbD"
      },
      "source": [
        "#load in specific partition of dataset based on client id\n",
        "def load_partition(idx: int):\n",
        "    \"\"\"Load 1/10th of the training and test data to simulate a partition.\"\"\"\n",
        "\n",
        "    emnist_train, emnist_test = tff.simulation.datasets.emnist.load_data(only_digits=True)\n",
        "    client_train = emnist_train.create_tf_dataset_for_client(emnist_train.client_ids[idx])\n",
        "    client_test = emnist_test.create_tf_dataset_for_client(emnist_test.client_ids[idx])\n",
        "\n",
        "    processed_train = preprocess(client_train, len(list(client_train)))\n",
        "    processed_test = preprocess(client_test, len(list(client_test)))\n",
        "\n",
        "    sample_train = tf.nest.map_structure(lambda x: x.numpy(),\n",
        "                                     next(iter(processed_train)))\n",
        "\n",
        "    sample_test = tf.nest.map_structure(lambda x: x.numpy(),\n",
        "                                     next(iter(processed_test)))\n",
        "\n",
        "    x_train = sample_train['x']\n",
        "    y_train = sample_train['y']\n",
        "\n",
        "    x_test = sample_test['x']\n",
        "    y_test = sample_test['y']\n",
        "\n",
        "    #Reshape to proper dimensions\n",
        "    x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "\n",
        "    #If number of overall training samples not evenly divided by batch size. Because every batch needs to be evenly divided by Microbatch_Num\n",
        "    if x_train.shape[0] % BATCH_SIZE != 0:\n",
        "      drop_num = x_train.shape[0] % BATCH_SIZE\n",
        "      x_train = x_train[:-drop_num]\n",
        "      y_train = y_train[:-drop_num]\n",
        "    \n",
        "    if x_train.shape[0] % BATCH_SIZE != 0:\n",
        "                raise ValueError(\n",
        "                    \"Batch Size should divide total train samples\"\n",
        "                )\n",
        "    x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
        "\n",
        "    \n",
        "    y_train = np.array(y_train, dtype=np.int32)\n",
        "    y_test = np.array(y_test, dtype=np.int32)\n",
        "\n",
        "    y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
        "    y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
        "\n",
        "    return (x_train,y_train),(x_test,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c81o1pEgpwf9"
      },
      "source": [
        "#Returns Model for Client and Server Side. SoftMax function at end removed for necessity\n",
        "def create_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    # 32 convolution filters used each of size 3x3\n",
        "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "    # 64 convolution filters used each of size 3x3\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "    # choose the best features via pooling\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    # randomly turn neurons on and off to improve convergence\n",
        "    model.add(Dropout(0.25))  \n",
        "    # flatten since too many dimensions, we only want a classification output\n",
        "    model.add(Flatten())\n",
        "    # fully connected to get all relevant data\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    # one more dropout\n",
        "    model.add(Dropout(0.5))\n",
        "    # output a softmax to squash the matrix into output probabilities\n",
        "    model.add(Dense(10))\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DIRtAoESUOx"
      },
      "source": [
        "#Centralized Model\n",
        "\n",
        "#preprocessing\n",
        "SHUFFLE_BUFFER = 100\n",
        "PREFETCH_BUFFER = 10\n",
        "\n",
        "NOISE_ARRAY = [.2, .4, .6, .8, 1, 1.2]\n",
        "\n",
        "l2_norm_clip = 1\n",
        "BATCH_SIZE = 32\n",
        "MICROBATCH_NUM = 8\n",
        "LEARNING_RATE = .1\n",
        "\n",
        "x_t = np.empty((0,28,28,1))\n",
        "y_t = np.empty((0,10))\n",
        "\n",
        "x_v = np.empty((0,28,28,1))\n",
        "y_v = np.empty((0,10))\n",
        "\n",
        "for i in range(100):\n",
        "\n",
        "  (x_train,y_train),(x_test,y_test) = load_partition(i)\n",
        "\n",
        "  x_t = np.append(x_t, x_train, axis=0)\n",
        "  y_t = np.append(y_t, y_train, axis=0)\n",
        "\n",
        "  x_v = np.append(x_v, x_test, axis=0)\n",
        "  y_v = np.append(y_v, y_test, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMJ6tIiIPcZy"
      },
      "source": [
        "epochs_for_noise = []\n",
        "val_accuracy_noise = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NbxfgkRCUaf"
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stop = EarlyStopping(monitor = 'val_accuracy', patience = 2)\n",
        "\n",
        "\n",
        "for noise in NOISE_ARRAY:\n",
        "  epochs = []\n",
        "  val_accuracy = []\n",
        "  model = create_model()\n",
        "\n",
        "  class MetricsCallback(Callback):    \n",
        "    def __init__(self, test_examples, test_data):\n",
        "        self.test_examples = test_examples\n",
        "        self.test_data = test_data\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs = None):\n",
        "        epochs.append(epoch)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs = None):\n",
        "        loss, accuracy = self.model.evaluate(self.test_examples, self.test_data, verbose = 1)\n",
        "        print(\"validation accuracy is\")\n",
        "        print(accuracy)\n",
        "        val_accuracy.append(accuracy)\n",
        "  \n",
        "  my_callback = MetricsCallback(test_examples = x_v, test_data=y_v)\n",
        "\n",
        "  print(noise)\n",
        "\n",
        "  optimizer =  VectorizedDPKerasSGDOptimizer(\n",
        "  l2_norm_clip=l2_norm_clip,\n",
        "  noise_multiplier=noise,\n",
        "  num_microbatches=MICROBATCH_NUM, learning_rate = LEARNING_RATE)\n",
        "\n",
        "  loss = tf.keras.losses.CategoricalCrossentropy(\n",
        "  from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
        "\n",
        "  model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "  model.fit(x_t, y_t, batch_size=BATCH_SIZE, epochs = 15, callbacks=[early_stop, my_callback], validation_data=(x_v, y_v))\n",
        "\n",
        "  epochs_for_noise.append(epochs)\n",
        "\n",
        "  val_accuracy_noise.append(val_accuracy)\n",
        "\n",
        "  print(epochs_for_noise, val_accuracy_noise)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzdBP8h01rcB"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "SHUFFLE_BUFFER = 100\n",
        "PREFETCH_BUFFER = 10\n",
        "\n",
        "l2_norm_clip = 1\n",
        "MICROBATCH_NUM = 8\n",
        "\n",
        "x_t = np.empty((0,28,28,1))\n",
        "y_t = np.empty((0,10))\n",
        "\n",
        "x_v = np.empty((0,28,28,1))\n",
        "y_v = np.empty((0,10))\n",
        "\n",
        "for i in range(100):\n",
        "\n",
        "  (x_train,y_train),(x_test,y_test) = load_partition(i)\n",
        "\n",
        "  x_t = np.append(x_t, x_train, axis=0)\n",
        "  y_t = np.append(y_t, y_train, axis=0)\n",
        "\n",
        "  x_v = np.append(x_v, x_test, axis=0)\n",
        "  y_v = np.append(y_v, y_test, axis=0)\n",
        "\n",
        "batch_sizes = [16, 32, 64, 128]\n",
        "learning_rates = [.1, .01, .001]\n",
        "\n",
        "p = np.random.permutation(len(x_t))\n",
        "\n",
        "x_t = x_t[p]\n",
        "y_t = y_t[p]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2PHVtyLThzt"
      },
      "source": [
        "def three_fold_splitter(split, x_t, y_t, b_size):\n",
        "  if split == 1:\n",
        "    x_train = x_t[:5376]\n",
        "    y_train = y_t[:5376]\n",
        "    x_test = x_t[5376:]\n",
        "    y_test = y_t[5376:]\n",
        "    if len(x_train) % b_size != 0:\n",
        "      drop_num = x_train.shape[0] % BATCH_SIZE\n",
        "      x_train = x_train[:-drop_num]\n",
        "      y_train = y_train[:-drop_num]\n",
        "    return (x_train, y_train), (x_test, y_test)\n",
        "  if split == 2:\n",
        "    x_train = x_t[2688:]\n",
        "    y_train = y_t[2688:]\n",
        "    x_test = x_t[:2688]\n",
        "    y_test = y_t[:2688]\n",
        "    if len(x_train) % b_size != 0:\n",
        "      drop_num = x_train.shape[0] % BATCH_SIZE\n",
        "      x_train = x_train[:-drop_num]\n",
        "      y_train = y_train[:-drop_num]\n",
        "    return (x_train, y_train), (x_test, y_test)\n",
        "  if split == 3:\n",
        "    x_train = np.concatenate((x_t[0:2688], x_t[5376:]))\n",
        "    y_train = np.concatenate((y_t[0:2688], y_t[5376:]))\n",
        "    x_test = x_t[2688:5376]\n",
        "    y_test = y_t[2688:5376]\n",
        "    if len(x_train) % b_size != 0:\n",
        "      drop_num = x_train.shape[0] % BATCH_SIZE\n",
        "      x_train = x_train[:-drop_num]\n",
        "      y_train = y_train[:-drop_num]\n",
        "    return (x_train, y_train), (x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOfz4jj5_anw"
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stop = EarlyStopping(monitor = 'val_loss', patience = 3, verbose = 1, restore_best_weights = True)\n",
        "\n",
        "validation_dict = {}\n",
        "\n",
        "for batch in batch_sizes:\n",
        "  for rate in learning_rates:\n",
        "    print(batch, rate)\n",
        "\n",
        "    pochs = []\n",
        "    losses = []\n",
        "\n",
        "    for split in [1, 2, 3]:\n",
        "\n",
        "      print(split)\n",
        "\n",
        "      (x_train, y_train), (x_val, y_val) = three_fold_splitter(split, x_t, y_t, batch)\n",
        "\n",
        "      model = create_model()\n",
        "        \n",
        "      my_callback = MetricsCallback(test_examples = x_v, test_data=y_v)\n",
        "\n",
        "      optimizer =  VectorizedDPKerasSGDOptimizer(\n",
        "      l2_norm_clip=l2_norm_clip,\n",
        "      noise_multiplier=.2,\n",
        "      num_microbatches=MICROBATCH_NUM, learning_rate = rate)\n",
        "\n",
        "      loss = tf.keras.losses.CategoricalCrossentropy(\n",
        "      from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
        "\n",
        "      model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "      model.fit(x_train, y_train, batch_size=batch, epochs = 15, callbacks=[early_stop], validation_data=(x_val, y_val))\n",
        "\n",
        "      epoch = early_stop.stopped_epoch\n",
        "\n",
        "      print(epoch)\n",
        "\n",
        "      loss, accuracy =  model.evaluate(x_val, y_val, verbose = 1)\n",
        "\n",
        "      print(loss)\n",
        "\n",
        "      pochs.append(epoch)\n",
        "      losses.append(loss)\n",
        "\n",
        "    average_loss = sum(losses)/3\n",
        "\n",
        "    validation_dict[(batch, rate)] = pochs, average_loss\n",
        "\n",
        "    print(validation_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WYf1aECcnfg"
      },
      "source": [
        "#Final Centralized Model Train\n",
        "import numpy as np\n",
        "\n",
        "l2_norm_clip = 1\n",
        "BATCH_SIZE = 32\n",
        "MICROBATCH_NUM = 8\n",
        "LEARNING_RATE = .1\n",
        "\n",
        "SHUFFLE_BUFFER = 100\n",
        "PREFETCH_BUFFER = 10\n",
        "\n",
        "x_t = np.empty((0,28,28,1))\n",
        "y_t = np.empty((0,10))\n",
        "\n",
        "x_v = np.empty((0,28,28,1))\n",
        "y_v = np.empty((0,10))\n",
        "\n",
        "for i in range(100):\n",
        "\n",
        "  (x_train,y_train),(x_test,y_test) = load_partition(i)\n",
        "\n",
        "  x_t = np.append(x_t, x_train, axis=0)\n",
        "  y_t = np.append(y_t, y_train, axis=0)\n",
        "\n",
        "  x_v = np.append(x_v, x_test, axis=0)\n",
        "  y_v = np.append(y_v, y_test, axis=0)\n",
        "\n",
        "p = np.random.permutation(len(x_t))\n",
        "\n",
        "x_t = x_t[p]\n",
        "y_t = y_t[p]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMASziaJfA_K"
      },
      "source": [
        "batch_test = []\n",
        "test_acc = []\n",
        "test_loss = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcJmf0vNfOKa"
      },
      "source": [
        "\n",
        "model = create_model()\n",
        "\n",
        "class MetricsCallback(tf.keras.callbacks.Callback):    \n",
        "  def __init__(self, test_examples, test_data):\n",
        "      self.test_examples = test_examples\n",
        "      self.test_data = test_data\n",
        "\n",
        "  def on_train_batch_end(self, batch, logs=None):\n",
        "        if batch % 140 == 0:\n",
        "          batch_test.append(batch)\n",
        "          loss, accuracy = self.model.evaluate(self.test_examples, self.test_data, verbose = 1)\n",
        "          test_acc.append(accuracy)\n",
        "          test_loss.append(loss)\n",
        "          print(batch_test)\n",
        "          print(test_acc)\n",
        "          print(test_loss)\n",
        "\n",
        "my_callback = MetricsCallback(test_examples = x_v, test_data=y_v)\n",
        "\n",
        "optimizer =  VectorizedDPKerasSGDOptimizer(\n",
        "l2_norm_clip=l2_norm_clip,\n",
        "noise_multiplier=.2,\n",
        "num_microbatches=MICROBATCH_NUM, learning_rate = LEARNING_RATE)\n",
        "\n",
        "\n",
        "loss = tf.keras.losses.CategoricalCrossentropy(\n",
        "from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_t, y_t, batch_size=32, epochs = 30, callbacks=[my_callback])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQCkI9KdDQGt",
        "outputId": "4801b290-83ad-4cdd-ab07-ecb4db0a5a37"
      },
      "source": [
        "\n",
        "compute_dp_sgd_privacy.compute_dp_sgd_privacy(n=60000,\n",
        "                                              batch_size = 32,\n",
        "                                              noise_multiplier=.2,\n",
        "                                              epochs=30,\n",
        "                                              delta=.00001)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DP-SGD with sampling rate = 0.0533% and noise_multiplier = 0.2 iterated over 56250 steps satisfies differential privacy with eps = 818 and delta = 1e-05.\n",
            "The optimal RDP order is 1.25.\n",
            "The privacy estimate is likely to be improved by expanding the set of orders.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(817.7613584737297, 1.25)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}